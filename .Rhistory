SD <- sensor_list %>% map_dfr(read_csv) %>% group_by(WD) %>% mutate(id=cur_group_id()) #combine all files
View(SD)
rm(SD)
SD <- sensor_list %>% map_dfr(read_csv) %>% group_by(WD) %>% mutate(id=cur_group_id()) #combine all files
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) #combine all files
SD1 <- SD %>% filter(time<=60)  #filtering out all data above the minute mark (60 seconds)
View(SD1)
View(SD1)
SD2 <- SD1 %>% select(-1,-2,-6)
View(SD2)
View(SD2)
rm(SD2)
SD1 <- SD %>% filter(time<=60) %>% ungroup() %>% select(3:5) #filtering out all data above the minute mark (60 seconds)
View(SD1)
Kmeans <- kmeans(SD1, 5)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wy)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 5)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wz)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 3)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wz)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 5)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wz)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 6)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wz)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 3)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wz)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 3)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wy)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 4)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y = wy)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
?princomp
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) #combine all files
SD1 <- SD %>% filter(time<=60) %>% ungroup() %>% select(2:5) #filtering out all data above the minute mark (60 seconds)
View(SD1)
Kmeans <- kmeans(SD1, 4)
Clust <- augment(Kmeans,SD1)
View(Clust)
p1 <-
ggplot(Clust, aes(x = time, y = wx)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
p1 <-
ggplot(Clust, aes(x = wx, y = time)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
Kmeans <- kmeans(SD1, 4)
Clust <- augment(Kmeans,SD1)
p1 <-
ggplot(Clust, aes(x = wx, y =wy)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
PCA <- princomp(SD1, scale.=TRUE)
summary(PCA)
loadings <- abs(pca$rotation)
loadings <- abs(PCA$rotation)
PCA$rotations
View(PCA)
PCA$loadings
PCA <- princomp(SD1, scale.=TRUE)
summary(PCA)
PCA$loadings
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) #combine all files
SD1 <- SD %>% filter(time<=60) %>% ungroup() %>% select(3:5) #filtering out all data above the minute mark (60 seconds)
PCA <- princomp(SD1, scale.=TRUE)
summary(PCA)
PCA$loadings
SD2 <- augment(PCA,data=SD)
library(broom)
SD2 <- augment(PCA,data=SD)
PCA <- prcomp(SD1, scale.=TRUE)
summary(PCA)
View(PCA)
View(PCA)
PCA$rotations
PCA$rotation
library(broom)
PCA <- prcomp(SD1, scale.=TRUE)
summary(PCA)
PCA$rotation
SD2 <- augment(PCA,data=SD)
SD2 <- augment(PCA,data=SD1)
library(broom)
PCA <- prcomp(SD1, scale.=TRUE)
summary(PCA)
PCA$rotation
SD2 <- augment(PCA,data=SD1)
View(SD2)
View(SD2)
Kmeans <- kmeans(SD2, 3)
Clust <- augment(Kmeans,SD2)
View(Clust)
View(Clust)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
SD3 <- SD2 %>% select(5:6)
Kmeans <- kmeans(SD3, 3)
Clust <- augment(Kmeans,SD3)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.5)
p1
View(SD3)
View(SD3)
View(SD)
View(SD)
biplot(SD2)
biplot(PCA)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SK <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) %>% filter(time<=60) #combine all files and filter anything under a minute or 60s
View(SK)
View(SK)
View(SK)
View(SK)
View(SD)
View(SD)
View(SD)
View(SD)
View(SD)
DF <- SD %>% ungroup() %>% select(2:6)#combine all files and filter anything under a minute or 60s
View(DF)
View(DF)
DF1 <- DF %>% as.character(time)
DF$time= as.character(DF$time)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
DF1 <- DF %>% pivot_wider(names_from = time, values_from = c(wx,wy,wz))
View(DF1)
DF1
View(DF1)
View(SD)
View(SD)
hist(SD$id)
hist(SD$time)
DF <- SD %>% ungroup() %>% select(2:6) %>% mutate(time=round(time,3))
View(DF)
DF
DF$time= as.character(DF$time)
DF1 <- DF %>% pivot_wider(names_from = time, values_from = c(wx,wy,wz))
DF1
View(DF1)
DF <- SD %>% ungroup() %>% select(2:6) %>% mutate(time=round(time,2))
View(DF)
DF <- SD %>% ungroup() %>% select(2:6) %>% mutate(time=round(time,2))
DF <- SD %>% ungroup() %>% select(2:6) %>% mutate(time=round(time,2)) %>% distinct()
DF$time= as.character(DF$time)
DF1 <- DF %>% pivot_wider(names_from = time, values_from = c(wx,wy,wz))
View(DF)
View(DF)
View(DF1)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
library(ggplot2) #to plot
library(GGally)#to plot correlations
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) %>% filter(time<=60) #combine all files and filter anything under a minute or 60s
SD1 <- SD %>% summarise(wx=mad(wx),wy=mad(wy),wz=mad(wz)) %>% ungroup()
SD2 <- SD1 %>% select(-1)
PCA <- prcomp(SD2, scale.=TRUE)# run PCA
summary(PCA)#get cumulative Proportion
PCA$rotation #Understand what each component is made up
SD3 <- augment(PCA,data=SD2) #create a frame with all the information
biplot(PCA)
plot(PCA,type="lines")
#From the biplot and the scree plot it makes sense to remove the 3rd components as PC1 and 2 account for more than 99% of the variance.
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 4)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
Outcomes <- Assignment_6_Outcome_data %>% slice(-8)
library(readxl)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
Taks2_outcomes <- augment(Kmeans,Outcomes)
View(SK)
View(SK)
View(Kmeans)
View(Clust)
View(Outcomes)
Taks2_outcomes <- augment(Kmeans,Outcomes)
View(Kmeans)
View(Kmeans)
View(Taks2_outcomes)
View(Taks2_outcomes)
ggcorr(T2O[,-1,-2], method = c("everything", "pearson"))
T2O <- augment(Kmeans,Outcomes)
View(T2O)
View(T2O)
ggcorr(T2O[,-1,-2], method = c("everything", "pearson"))
View(T2O)
ggcorr(T2O[,-1:2,], method = c("everything", "pearson"))
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
T2O$.cluster=as.numeric(t2O$cluster)
T2O$.cluster=as.numeric(T2O$cluster)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
View(Clust)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Clust,Outcomes)
T2O <- augment(Kmeans,Outcomes)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
cor <- cor(T2O[,-1:-2,], method = "spearman" )
cor
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
View(Clust)
View(Clust)
View(T2O)
View(T2O)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "spearman" )
cor
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 2)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
#There seems to be a moderate negative correlation between the clusters and the number of number of jumps
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
#There seems to be a moderate negative correlation between the clusters and the number of number of jumps
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1
View(T2O)
View(T2O)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
#There seems to be a moderate negative correlation between the clusters and the number of number of jumps
View(Clust)
View(Clust)
View(T2O)
View(T2O)
View(T2O)
View(T2O)
boxplot(.cluster~Jumps, data=t2O)
boxplot(.cluster~Jumps, data=T2O)
boxplot(Jumps~.cluster, data=T2O)
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
#There seems to be a moderate correlation (-.58) between the clusters and the number of number of jumps.
View(SD1)
View(SD1)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
#using google drive to download files from Google Drive
#folder_url <- "https://drive.google.com/drive/folders/1am3wIQPXFvvFAmsJG2m85HLl0G63rJE4?usp=sharing" #This is the Googledrive folder with files
#folder <- drive_get(as_id(folder_url))
#csv_files <- drive_ls(folder, type = "csv")
#walk(csv_files$id, ~ drive_download(as_id(.x), overwrite = TRUE))
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) %>% filter(time<=60) #combine all files and filter anything under a minute or 60s
SD1 <- SD %>% summarise(wx=mad(wx),wy=mad(wy),wz=mad(wz)) %>% ungroup()
SD2 <- SD1 %>% select(-1)
PCA <- prcomp(SD2, scale.=TRUE)# run PCA
summary(PCA)#get cumulative Proportion
PCA$rotation #Understand what each component is made up
SD3 <- augment(PCA,data=SD2) #create a frame with all the information
biplot(PCA)
plot(PCA,type="lines")
#From the biplot and the scree plot it makes sense to remove the 3rd components as PC1 and 2 account for more than 99% of the variance.
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1 #basically beautifying the biplot
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
library(readxl)
#using google drive to download files from Google Drive
#folder_url <- "https://drive.google.com/drive/folders/1am3wIQPXFvvFAmsJG2m85HLl0G63rJE4?usp=sharing" #This is the Googledrive folder with files
#folder <- drive_get(as_id(folder_url))
#csv_files <- drive_ls(folder, type = "csv")
#walk(csv_files$id, ~ drive_download(as_id(.x), overwrite = TRUE))
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) %>% filter(time<=60) #combine all files and filter anything under a minute or 60s
SD1 <- SD %>% summarise(wx=mad(wx),wy=mad(wy),wz=mad(wz)) %>% ungroup()
SD2 <- SD1 %>% select(-1)
PCA <- prcomp(SD2, scale.=TRUE)# run PCA
summary(PCA)#get cumulative Proportion
PCA$rotation #Understand what each component is made up
SD3 <- augment(PCA,data=SD2) #create a frame with all the information
biplot(PCA)
plot(PCA,type="lines")
#From the biplot and the scree plot it makes sense to remove the 3rd components as PC1 and 2 account for more than 99% of the variance.
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1 #basically beautifying the biplot
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse) #to clean and prep data
library(tidymodels) # to access some of the tidy model tools
library(janitor) # to clean data
library(googledrive)#to get files from Google drive
library(broom)#to extract the PCs and Clusters
library(readxl)
library(ggplot2)
library(GGally)
library(factoextra)
library(dplyr)
#using google drive to download files from Google Drive
#folder_url <- "https://drive.google.com/drive/folders/1am3wIQPXFvvFAmsJG2m85HLl0G63rJE4?usp=sharing" #This is the Googledrive folder with files
#folder <- drive_get(as_id(folder_url))
#csv_files <- drive_ls(folder, type = "csv")
#walk(csv_files$id, ~ drive_download(as_id(.x), overwrite = TRUE))
sensor_list <- list.files(pattern="*.csv")#creating a list of sensor files
WD <- getwd()#get working directory to combine files
SD <- sensor_list %>% map_dfr(read_csv,.id = "WD") %>% group_by(WD) %>% mutate(id=cur_group_id()) %>% filter(time<=60) #combine all files and filter anything under a minute or 60s
SD1 <- SD %>% summarise(wx=mad(wx),wy=mad(wy),wz=mad(wz)) %>% ungroup()
SD2 <- SD1 %>% select(-1)
PCA <- prcomp(SD2, scale.=TRUE)# run PCA
summary(PCA)#get cumulative Proportion
PCA$rotation #Understand what each component is made up
SD3 <- augment(PCA,data=SD2) #create a frame with all the information
biplot(PCA)
plot(PCA,type="lines")
#From the biplot and the scree plot it makes sense to remove the 3rd components as PC1 and 2 account for more than 99% of the variance.
KT <- SD3 %>% select(5:6)
Kmeans <- kmeans(KT, 3)
Clust <- augment(Kmeans,KT)
p1 <-
ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx))
p1 #basically beautifying the biplot
Outcomes <- read_excel("Assignment  6 Outcome data.xlsx")
T2O <- augment(Kmeans,Outcomes)
T2O$.cluster=as.numeric(T2O$.cluster)
ggcorr(T2O[,-1:-2,], method = c("everything", "pearson"))
cor <- cor(T2O[,-1:-2,], method = "pearson" )
cor
#There seems to be a moderate correlation (-.58) between the clusters and the number of number of jumps.
library(ggplot2)
library(GGally)
library(factoextra)
library(dplyr)
S1 = read.csv("surveydata.csv", header = T)
ggpairs(S1, 2:6, progress = FALSE)
ggcorr(S1[,-1], method = c("everything", "pearson"))
drop <- c("Name")
S2 <- S1[,!(names(S1) %in% drop)]
pcas <- prcomp(S2, scale. = TRUE)
pcas$sdev
#To convert this into variance accounted for we can square it, these numbers are proportional to the eigenvalue
pcas$sdev^2
#A summary of our pca will give us the proportion of variance accounted for by each component
summary(pcas)
#We can look at this to get an idea of which components we should keep and which we should drop
plot(pcas, type = "lines",ylim=c(0,3))
pcas$rotation
loadings <- abs(pcas$rotation) #abs() will make all eigenvectors positive
head(summary(pcas))
V1 = pcas$x
V2 <- V1
rownames(V2)<-S1$Name
V3 <- as.data.frame(V2)
names <- c("Sara","Yuan","Yurui","Jingshu","Jie","Shuyi","Manrui","Xingyi")
barplot(V3$PC1, ylab = "Each member's score on PC1", ylim=c(-4,4), names.arg=names, cex.names=1)
p2 <- ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx)) +
labs(x="Move Up/Down & Left/Right",y="Move Front/Back",title = "Movement of Jumping")
p2
jumps <- Task2_outcomes$Jumps
p2 <- ggplot(Clust, aes(x =.fittedPC1 , y =.fittedPC2)) +
geom_point(aes(color = .cluster), alpha = 0.8, size=1.5*(SD1$wx)) +
labs(x="Move Up/Down & Left/Right",y="Move Front/Back",title = "Movement of Jumping")
p2
jumps <- T2O$Jumps
p3 <- ggplot(data = T2O, aes(Name,Jumps)) +geom_point(aes(color = .cluster), alpha = 0.8,pch=3,cex=4)
p3
